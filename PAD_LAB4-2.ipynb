{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7b4f6380",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import re\n",
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from functools import reduce\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pymorphy3\n",
    "from pymystem3 import Mystem\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7f74d9e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ü—Ä–æ—Ü–µ–Ω—Ç negative –≤ train:  0.5773692274020573\n",
      "–ü—Ä–æ—Ü–µ–Ω—Ç negative –≤ test:  0.5774278215223098\n",
      "                                                   text     label\n",
      "0     –ü–æ–º–æ–π–º—É —è –≤–∫—Ä–∞—à–∏–ª–∞—Å—å –≤ –ß–∏–º–∏–Ω–∞ü§ß https://t.co/t2...  positive\n",
      "1                         @buybread_ —è –Ω–µ —Å –ø–æ—Ä—è–¥–∫–µ!!!!  negative\n",
      "2     @ange1flyhigh –í —Å–ª–µ–¥—É—é—â–∏–π —Ä–∞–∑ –±—É–¥—É –¥–æ –ø–æ–±–µ–¥–Ω–æ–≥...  positive\n",
      "3     @LimitaVIP –£–¥–∏–≤–∏—Ç–µ–ª—å–Ω—ã–π –≥i–º–Ω...\\r\\n–£–¥–∏–≤–∏—Ç–µ–ª—å–Ω–æ...  negative\n",
      "4                               —è —Å—Ä–∞–ª–∞ –Ω–∞ —ç—Ç—É –±–∏–æ–ª–æ–≥–∏—é  negative\n",
      "...                                                 ...       ...\n",
      "4564  –î–µ–¥—Ä–∞–¥–∏–æ5 —Ç–æ –µ—Å—Ç—å —Ç—ã —Ö–æ—á–µ—à—å —Å–∫–∞–∑–∞—Ç—å, —á—Ç–æ —Ç—ã –ª–∞...  negative\n",
      "4565  @kmoo_m –î–ï–ô–°–¢–í–ò–¢–ï–õ–¨–ù–û\\r\\n–µ—Å–ª–∏ –ª—é–±–æ–≤—å, —Ç–æ —Ç–æ–ª—å–∫...  positive\n",
      "4566  –° –•–æ–±–∏ —É—Ç—Ä–æ –º–æ–∂–µ—Ç –±—ã—Ç—å —Ç–æ–ª—å–∫–æ –¥–æ–±—Ä—ã–ºüòÇ https://...  positive\n",
      "4567  –ù–µ —É—Å–ø–µ–ª–∞ –≤—Å—Ç–∞—Ç—å, –∞ —É–∂–µ –∑–∞–µ–±–∞–ª–∞—Å—å, –≤–ø—Ä–æ—á–µ–º –Ω–∏—á...  negative\n",
      "4568  –∞—Ö —Ç–≤–∏—Ç—Ç–µ—Ä –∏–≤–∞–Ω–∞ —É—Ä–≥–∞–Ω—Ç–∞, –∞ –≤–µ–¥—å –≤—Å—ë —Ç–∞–∫ —Ö–æ—Ä–æ—à...  negative\n",
      "\n",
      "[4569 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df2 = pd.DataFrame(columns=['text', 'label'])\n",
    "df1 = pd.DataFrame(columns=['text', 'label'])\n",
    "df_lem_train = pd.DataFrame(columns=['text', 'label'])\n",
    "df_lem_test = pd.DataFrame(columns=['text', 'label'])\n",
    "\n",
    "cou = 0\n",
    "\n",
    "# –¥–æ—Å—Ç–∞–µ–º –æ–±—É—á–∞—é—â—É—é –≤—ã–±–æ—Ä–∫—É –∏ –∑–∞–≥—Ä—É–∂–∞–µ–º –≤ 2 –î–∞—Ç–∞–§—Ä–µ–π–º–∞: –¥–ª—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –∏ —Å—Ç–µ–º–º–∏–Ω–≥–∞\n",
    "df_train = pd.read_csv('/Users/mac/Downloads/rusentitweet_train.csv')\n",
    "for i, row in df_train.iterrows():\n",
    "    if row[1] in (\"negative\", \"positive\"):\n",
    "        if row[1]=='negative':\n",
    "            cou+=1\n",
    "        df2.loc[len(df2.index)] = row\n",
    "        df_lem_train.loc[len(df_lem_train.index)] = row\n",
    "        \n",
    "df_train = df2\n",
    "print('–ü—Ä–æ—Ü–µ–Ω—Ç negative –≤ train: ', cou/df_train.shape[0])\n",
    "        \n",
    "cou = 0\n",
    "# –¥–æ—Å—Ç–∞–µ–º —Ç–µ—Å—Ç–æ–≤—É –≤—ã–±–æ—Ä–∫—É –∏ –∑–∞–≥—Ä—É–∂–∞–µ–º –≤ 2 –î–∞—Ç–∞–§—Ä–µ–π–º–∞: –¥–ª—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –∏ —Å—Ç–µ–º–º–∏–Ω–≥–∞\n",
    "df_test = pd.read_csv('/Users/mac/Downloads/rusentitweet_test.csv')\n",
    "for i, row in df_test.iterrows():\n",
    "    if row[1] in (\"negative\", \"positive\"):\n",
    "        df1.loc[len(df1.index)] = row\n",
    "        if row[1]=='negative':\n",
    "            cou+=1\n",
    "        df_lem_test.loc[len(df_lem_test.index)] = row\n",
    "df_test = df1\n",
    "print('–ü—Ä–æ—Ü–µ–Ω—Ç negative –≤ test: ', cou/df_test.shape[0])\n",
    "\n",
    "\n",
    "print(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "82dae75b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/mac/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "# –°—Ç–æ–ø-—Å–ª–æ–≤–∞\n",
    "stop = stopwords.words('russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d16ceda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 –§—É–Ω–∫—Ü–∏—è –æ—á–∏—Å—Ç–∫–∏\n",
    "def my_clear(df):\n",
    "    s=[]\n",
    "    s1 = \"\"\n",
    "    regs = [r'https?:\\/\\/.\\S+', r'#\\S+', r'@\\S+', r'\\\\r\\\\n', r'[-.)(,:]']\n",
    "    glasn = ['–æ', '–∞', '—è', '–∞', '–∏', '–µ', '—ã']\n",
    "    signs = ['!', '_', '.']\n",
    "\n",
    "    # –ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –≤—Å–µ–≥–æ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É\n",
    "    for i, row in df.iterrows():\n",
    "        row[0] = row[0].lower()\n",
    "\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        # —É–¥–∞–ª–µ–Ω–∏–µ —Å—Å—ã–ª–æ, —Ö—ç—à—Ç–µ–≥–æ–≤(–∏ –∑–Ω–∞–∫–∞ —Ä–µ—à–µ—Ç–∫–∏ –∏ –ø–æ—Å–ª–µ–¥—É—é—â–µ–≥–æ —Ö—ç—à—Ç–µ–≥–∞, –ø–æ—Ç–æ–º—É —á—Ç–æ —Å–ª–æ–≤–∞ –ø–æ—Å–ª–µ —Ö—ç—à—Ç—ç–≥–∞ –ø–∏—à—É—Ç—Å—è —Å–ª–∏—Ç–Ω–æ)\n",
    "        # —É–¥–∞–ª–µ–Ω–∏–µ –æ—Ç–º–µ—á–µ–Ω–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π(@name), —É–¥–∞–ª–µ–Ω–∏–µ –∑–Ω–∞–∫–æ–≤ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏ (–∫—Ä–æ–º–µ ! ?)\n",
    "        for reg in regs:\n",
    "            row[0] = re.sub(reg, \"\", row[0])\n",
    "\n",
    "        # —É–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤\n",
    "        row[0] = ' '.join([x for x in row[0].split() if x not in stop])\n",
    "\n",
    "        # —É–¥–∞–ª–µ–Ω–∏–µ –ø–æ–≤—Ç–æ—Ä—è—â–∏—Ö—Å—è –ø–æ–¥—Ä—è–¥ —Å–º–∞–π–ª–∏–∫–æ–≤ –∏ –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è (–æ—Å—Ç–∞–≤—à–∏—Ö—Å—è)\n",
    "        # —É–¥–∞–ª–µ–Ω–∏–µ –ø–æ–¥—Ä—è–¥ –∏–¥—É—â–∏—Ö –≥–ª–∞—Å–Ω—ã—Ö (—Ç—ã—ã—ã—ã—ã, —è—è—è—è—è—è, –∞–∞–∞–∞–∞–∞ –∏ —Ç–¥)\n",
    "        # —É–¥–∞–ª–µ–Ω–∏–µ —Ü–∏—Ñ—Ä, —Ç–∞–∫ –∫–∞–∫ –ø–æ —Å—É—Ç–∏ —Ü–∏—Ñ—Ä—ã –Ω–µ –∏–º–µ—é—Ç –Ω–∏–∫–∞–∫–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è –≤ –¥–∞–Ω–Ω–æ–º –∞–Ω–∞–ª–∏–∑–µ\n",
    "        r = row[0]\n",
    "        for t in range(0, len(row[0])):\n",
    "            if not(not(r[t].isalpha()) and r[t] == r[t-1]) and not((r[t] in glasn) and r[t] == r[t-1]) and not(r[t].isdigit()):\n",
    "                s1+=r[t]\n",
    "        row[0] = s1\n",
    "\n",
    "        s1 = \"\"\n",
    "\n",
    "        r = row[0] + \"  \"\n",
    "\n",
    "        #–æ—Ç–¥–µ–ª–µ–Ω–∏–µ —Å–º–∞–π–ª–∏–∫–∞ –æ—Ç —Å–ª–æ–≤–∞ (—á–∏–º–∏–Ω–∞ü§ß -> —á–∏–º–∏–Ω–∞ ü§ß)\n",
    "        for t in range(0, len(row[0])):\n",
    "            if not(r[t+1].isalpha()) and r[t]!= \" \" and not(r[t+1].isdigit()) and (r[t+2].isalpha() or r[t+2]==\" \"):\n",
    "                s1+=r[t] + \" \"\n",
    "            else:\n",
    "                s1+=r[t]\n",
    "        row[0] = s1\n",
    "\n",
    "        s = []\n",
    "        s1=\"\"\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f28a630d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 –æ—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "df_train = my_clear(df_train)\n",
    "df_test = my_clear(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "dc99ba44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3\n",
    "# —Å—Ç–µ–º–º–∏–Ω–≥ –æ–±—É—á–∞—é—â–∏—Ö –∏ —Ç–µ—Å—Ç–æ–≤—ã—Ö\n",
    "new = SnowballStemmer(language='russian')\n",
    "for i, row in df_test.iterrows():\n",
    "    row[0] = ' '.join([new.stem(x) for x in row[0].split()])\n",
    "    \n",
    "for i, row in df_train.iterrows():\n",
    "    row[0] = ' '.join([new.stem(x) for x in row[0].split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f5886558",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['–ø–æ–º–æ–π–º –≤–∫—Ä–∞—à —á–∏–º–∏–Ω ü§ß',\n",
       " '–ø–æ—Ä—è–¥–∫ !',\n",
       " '—Å–ª–µ–¥ –±—É–¥ –ø–æ–±–µ–¥–Ω –∑–∞–∫—Ä—ã–≤–∞ –ø–æ–∂–µ–ª–∞ —É–¥–∞—á !',\n",
       " '—É–¥–∏–≤–∏—Ç–µ–ª—å–Ω –≥–∏–º–Ω —É–¥–∏–≤–∏—Ç–µ–ª—å–Ω –ø–æ–∫ –µ—â —Å–¥–æ—Ö–ª —É–∫—Ä–∞–∏–Ω',\n",
       " '—Å—Ä–∞–ª –±–∏–æ–ª–æ–≥',\n",
       " '–ø–æ–º–∏–º –∞–ª–∏–Ω –µ—â —Ä–∞–¥–æ—Å—Ç –≥–µ–º–æ–≥–ª–æ–±–∏–Ω –ø–æ–≤—ã—Å',\n",
       " '–ø–∏–∑–¥–µ—Ü —á–µ –≤–æ–±—â —á—É–≤—Å—Ç–≤ –∂–∏–∑–Ω –ø–æ–º–æ—Ç–∞ –ø—Ä–æ—à–ª —Å—É—Ç–∫ —Ç—É–º–∞–Ω',\n",
       " '—Å–ø–∏—á–∫ ? –∑–∞–∂–µ—á –æ–≥–æ–Ω —Ç–≤–æ –≥–ª–∞–∑',\n",
       " '—ç—Ç —Å–∞–º –¥–µ–ª –æ—á–µ–Ω –∫—Ä—É—Ç',\n",
       " '—Ö–æ—á —Å–∫–∞–∑–∞ —á—Ç–æ—Ç –ø—Ä–∏—è—Ç–Ω –ø–æ–ª—É—á–∞ \"–∏–¥ –Ω–∞—Ö –µ–±–ª–∞ –ø—Ä–æ—Ç–∏–≤–Ω \"']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = []\n",
    "x_test = []\n",
    "\n",
    "# —Å–æ–∑–¥–∞–Ω–∏–µ –º–∞—Å—Å–∏–≤–∞ —Å—Ç—Ä–æ–∫\n",
    "for i, row in df_train.iterrows():\n",
    "    x_train.append(row[0])\n",
    "\n",
    "for i, row in df_test.iterrows():\n",
    "    x_test.append(row[0])\n",
    "x_train[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "d231157f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4\n",
    "vec = TfidfVectorizer(stop_words=None, min_df=0.0, ngram_range=(1,4))\n",
    "\n",
    "# tf-idf test and train\n",
    "matr = vec.fit_transform(x_train)\n",
    "matr_test = vec.transform(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1853b3db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3.91, 0.773403324584427), (3.81, 0.773403324584427), (3.7100000000000004, 0.773403324584427), (3.6100000000000003, 0.7716535433070866), (3.21, 0.7707786526684165), (3.41, 0.7699037620297463), (3.31, 0.7699037620297463), (2.91, 0.7699037620297463), (3.5100000000000002, 0.7690288713910761), (3.1100000000000003, 0.7690288713910761), (3.0100000000000002, 0.7690288713910761), (2.81, 0.7690288713910761), (2.6100000000000003, 0.7690288713910761), (2.71, 0.7681539807524059), (2.5100000000000002, 0.7681539807524059), (2.41, 0.7664041994750657), (2.31, 0.7646544181977253), (2.1100000000000003, 0.7646544181977253), (1.81, 0.7646544181977253), (2.21, 0.7629046369203849), (2.0100000000000002, 0.7629046369203849), (1.9100000000000001, 0.7620297462817148), (1.7100000000000002, 0.7611548556430446), (1.51, 0.7602799650043744), (1.61, 0.7585301837270341), (1.4100000000000001, 0.7567804024496938), (1.31, 0.7567804024496938), (1.21, 0.7532808398950132), (0.81, 0.7532808398950132), (1.11, 0.752405949256343), (1.01, 0.7515310586176728), (0.91, 0.7506561679790026), (0.7100000000000001, 0.7462817147856518), (0.61, 0.7410323709536308), (0.51, 0.731408573928259), (0.41000000000000003, 0.7200349956255468), (0.31, 0.7147856517935258), (0.21000000000000002, 0.6850393700787402), (0.11, 0.6299212598425197), (0.01, 0.5774278215223098)]\n",
      "[(3.91, 0.9938717443641935), (3.81, 0.9938717443641935), (3.7100000000000004, 0.9936528780914861), (3.6100000000000003, 0.9932151455460714), (3.5100000000000002, 0.9932151455460714), (3.41, 0.9932151455460714), (3.31, 0.9925585467279492), (3.21, 0.9925585467279492), (3.1100000000000003, 0.9921208141825345), (3.0100000000000002, 0.9919019479098271), (2.91, 0.9919019479098271), (2.81, 0.9914642153644123), (2.71, 0.9914642153644123), (2.6100000000000003, 0.9914642153644123), (2.5100000000000002, 0.9910264828189976), (2.41, 0.9905887502735828), (2.31, 0.990151017728168), (2.21, 0.9897132851827534), (2.1100000000000003, 0.9892755526373386), (2.0100000000000002, 0.9877434887283869), (1.9100000000000001, 0.9875246224556796), (1.81, 0.9857736922740206), (1.7100000000000002, 0.9846793609104837), (1.61, 0.9829284307288247), (1.51, 0.9800831691836288), (1.4100000000000001, 0.9776756401838477), (1.31, 0.9735171810024076), (1.21, 0.9676077916393084), (1.11, 0.9614795360035019), (1.01, 0.953381483913329), (0.91, 0.9413438389144233), (0.81, 0.9188006128255636), (0.7100000000000001, 0.8772160210111621), (0.61, 0.8456992777413), (0.51, 0.8025826220179471), (0.41000000000000003, 0.7557452396585687), (0.31, 0.7089078572991901), (0.21000000000000002, 0.6572554169402495), (0.11, 0.6104180345808711), (0.01, 0.5773692274020573)]\n"
     ]
    }
   ],
   "source": [
    "c = {}\n",
    "train = {}\n",
    "\n",
    "for i in np.arange(0.01, 4, 0.1):\n",
    "    model_logreg = LogisticRegression(C=i)\n",
    "    model_logreg.fit(matr, df_train['label'])\n",
    "    y_logreg = model_logreg.predict(matr_test)\n",
    "    model_logreg.fit(matr, df_train['label'])\n",
    "    accuracy = accuracy_score(df_test['label'], y_logreg)\n",
    "    y_logreg = model_logreg.predict(matr)\n",
    "    accuracy1 = accuracy_score(df_train['label'], y_logreg)\n",
    "    \n",
    "    c[i] = accuracy \n",
    "    train[i] = accuracy1\n",
    "    \n",
    "c = sorted(c.items(), key=lambda x: x[1])\n",
    "c = c[::-1]\n",
    "train = sorted(train.items(), key=lambda x: x[1])\n",
    "train = train[::-1]\n",
    "\n",
    "\n",
    "print(c)\n",
    "print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "defa94d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7089078572991901\n"
     ]
    }
   ],
   "source": [
    "# 5 logreg\n",
    "model_logreg = LogisticRegression(C=.31)\n",
    "model_logreg.fit(matr, df_train['label'])\n",
    "y_logreg = model_logreg.predict(matr)\n",
    "\n",
    "accuracy = accuracy_score(df_train['label'], y_logreg)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "738254df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7147856517935258\n"
     ]
    }
   ],
   "source": [
    "model_logreg = LogisticRegression(C=0.31)\n",
    "model_logreg.fit(matr, df_train['label'])\n",
    "y_logreg = model_logreg.predict(matr_test)\n",
    "\n",
    "accuracy = accuracy_score(df_test['label'], y_logreg)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "54bbb4b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['–ª—é–±–ª', '–∫—Ä–∞—Å–∏–≤', '–±–ª—è—Ç', '–ø–∏–∑–¥–µ—Ü', '–º–∏–ª', '–∫—Ä—É—Ç', '–≤–∞',\n",
       "       '–ø—Ä–µ–∫—Ä–∞—Å–Ω', '–ª—É—á—à', '—Ö–æ—Ä–æ—à'], dtype=object)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5 top logreg\n",
    "importance = model_logreg.coef_\n",
    "abs_importance = abs(importance)\n",
    "sorted_index = abs_importance.argsort()\n",
    "sorted_index = sorted_index[0][::-1]\n",
    "# —Ç–æ–ø-10 –∑–Ω–∞—á–∏–º—ã—Ö —Å–ª–æ–≤ (–≥–æ—Ä—è—á–∞—è –¥–µ—Å—è—Ç–∫–∞)\n",
    "np.array(vec.get_feature_names_out())[sorted_index[0:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6c3e0a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(48, 0.6867891513560805), (54, 0.6850393700787402), (53, 0.6824146981627297), (47, 0.6806649168853893), (52, 0.6780402449693789), (51, 0.6745406824146981), (50, 0.673665791776028), (46, 0.673665791776028), (49, 0.6719160104986877), (45, 0.6710411198600175), (44, 0.6710411198600175), (42, 0.6701662292213474), (37, 0.6692913385826772), (38, 0.6640419947506562), (39, 0.6587926509186351), (40, 0.657917760279965), (41, 0.6570428696412949), (36, 0.6570428696412949), (43, 0.6552930883639545), (32, 0.6465441819772528), (31, 0.6447944006999126), (34, 0.6404199475065617), (35, 0.6351706036745407), (33, 0.6342957130358705), (30, 0.6334208223972003), (26, 0.6307961504811899), (28, 0.6229221347331584), (29, 0.615923009623797), (27, 0.6132983377077865), (25, 0.6115485564304461)]\n",
      "[(54, 0.7393302692055155), (53, 0.7380170715692712), (51, 0.7301378857518056), (52, 0.7277303567520245), (50, 0.7211643685708032), (49, 0.7178813744801926), (46, 0.7172247756620704), (48, 0.7167870431166558), (47, 0.712847450207923), (44, 0.7108776537535566), (45, 0.701247537754432), (42, 0.7008098052090173), (43, 0.6900853578463558), (41, 0.6898664915736485), (38, 0.6878966951192821), (40, 0.6815495732107683), (37, 0.6815495732107683), (39, 0.6806741081199387), (35, 0.666885532939374), (36, 0.6631648063033486), (32, 0.654629021667761), (33, 0.6520026263952725), (34, 0.6517837601225651), (31, 0.6502516962136135), (28, 0.6421536441234406), (29, 0.6397461151236594), (30, 0.6355876559422193), (27, 0.6294594003064128), (26, 0.6263952724885096), (25, 0.6154519588531407)]\n"
     ]
    }
   ],
   "source": [
    "c = {}\n",
    "train = {}\n",
    "\n",
    "for i in range(25, 55):\n",
    "    model_random_forest = RandomForestClassifier(max_depth=i)\n",
    "    model_random_forest.fit(matr, df_train['label'])\n",
    "    y_logreg = model_random_forest.predict(matr_test)\n",
    "    model_random_forest.fit(matr, df_train['label'])\n",
    "    accuracy = accuracy_score(df_test['label'], y_logreg)\n",
    "    y_logreg = model_random_forest.predict(matr)\n",
    "    accuracy1 = accuracy_score(df_train['label'], y_logreg)\n",
    "    \n",
    "    c[i] = accuracy \n",
    "    train[i] = accuracy1\n",
    "    \n",
    "c = sorted(c.items(), key=lambda x: x[1])\n",
    "c = c[::-1]\n",
    "train = sorted(train.items(), key=lambda x: x[1])\n",
    "train = train[::-1]\n",
    "\n",
    "\n",
    "print(c)\n",
    "print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "0cd0a3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6596629459400306\n"
     ]
    }
   ],
   "source": [
    "# 5 random_forest\n",
    "model_random_forest = RandomForestClassifier(max_depth=35)\n",
    "model_random_forest.fit(matr, df_train['label'])\n",
    "y_forest = model_random_forest.predict(matr)\n",
    "\n",
    "accuracy = accuracy_score(df_train['label'], y_forest)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "4998d407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6622922134733158\n"
     ]
    }
   ],
   "source": [
    "model_random_forest = RandomForestClassifier(max_depth=35)\n",
    "model_random_forest.fit(matr, df_train['label'])\n",
    "y_forest = model_random_forest.predict(matr_test)\n",
    "\n",
    "accuracy = accuracy_score(df_test['label'], y_forest)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "c64f43ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['–∫—Ä–∞—Å–∏–≤', '–ª—É—á—à', '–±–ª—è—Ç', '–ø–∏–∑–¥–µ—Ü', '–º–∏–ª', '–ª—é–±–ª', '–ø—Ä–µ–∫—Ä–∞—Å–Ω',\n",
       "       '–∫—Ä—É—Ç', '—Ö–æ—Ä–æ—à', '–Ω–∞—Ö'], dtype=object)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5 top random_forest\n",
    "importance = model_random_forest.feature_importances_\n",
    "abs_importance = abs(importance)\n",
    "sorted_index = abs_importance.argsort()[::-1]\n",
    "# —Ç–æ–ø-10 –∑–Ω–∞—á–∏–º—ã—Ö —Å–ª–æ–≤\n",
    "np.array(vec.get_feature_names_out())[sorted_index[0:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ba4e63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "931e7eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/mac/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/mac/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.stem import WordNetLemmatizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "ad61d6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear\n",
    "df_lem_train = my_clear(df_lem_train)\n",
    "df_lem_test = my_clear(df_lem_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "5b4a726f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mystem - –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –ª–µ–º–º–∞—Ç–∏–∑–∞—Ç–æ—Ä + pip install pymorphy3-dicts-ru - —Å–ª–æ–≤–∞—Ä–∏\n",
    "m = Mystem()\n",
    "\n",
    "x_test_lem = []\n",
    "x_train_lem = []\n",
    "\n",
    "# lemmatize\n",
    "for i, t in df_lem_train.iterrows():\n",
    "    lemmas = m.lemmatize(t[0])\n",
    "    x_train_lem.append(\"\".join(lemmas).strip())\n",
    "    \n",
    "for i, t in df_lem_test.iterrows():\n",
    "    lemmas = m.lemmatize(t[0])\n",
    "    x_test_lem.append(\"\".join(lemmas).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "902ea45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = TfidfVectorizer(stop_words=None)\n",
    "\n",
    "# tf-idf test and train\n",
    "matr_train_lem = vec.fit_transform(x_train_lem)\n",
    "matr_test_lem = vec.transform(x_test_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "85c33b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2.31, 0.7865266841644795), (3.0100000000000002, 0.7856517935258093), (2.91, 0.7856517935258093), (2.6100000000000003, 0.7856517935258093), (2.41, 0.7856517935258093), (3.6100000000000003, 0.7847769028871391), (3.31, 0.7847769028871391), (3.21, 0.7847769028871391), (2.81, 0.7847769028871391), (2.71, 0.7847769028871391), (2.5100000000000002, 0.7847769028871391), (3.81, 0.7839020122484689), (3.7100000000000004, 0.7839020122484689), (3.41, 0.7839020122484689), (3.1100000000000003, 0.7839020122484689), (2.21, 0.7839020122484689), (1.9100000000000001, 0.7839020122484689), (3.91, 0.7830271216097988), (3.5100000000000002, 0.7830271216097988), (2.1100000000000003, 0.7830271216097988), (2.0100000000000002, 0.7830271216097988), (1.7100000000000002, 0.7830271216097988), (1.81, 0.7821522309711286), (1.61, 0.7804024496937882), (1.4100000000000001, 0.7795275590551181), (1.51, 0.778652668416448), (1.31, 0.778652668416448), (1.11, 0.7769028871391076), (1.01, 0.7769028871391076), (1.21, 0.7760279965004374), (0.91, 0.7725284339457568), (0.81, 0.7707786526684165), (0.7100000000000001, 0.7646544181977253), (0.61, 0.7646544181977253), (0.51, 0.7559055118110236), (0.41000000000000003, 0.7427821522309711), (0.31, 0.7305336832895888), (0.21000000000000002, 0.7147856517935258), (0.11, 0.663167104111986), (0.01, 0.5774278215223098)]\n",
      "[(3.91, 0.9684832567301379), (3.81, 0.9680455241847231), (3.7100000000000004, 0.967388925366601), (3.6100000000000003, 0.9671700590938936), (3.5100000000000002, 0.9669511928211862), (3.41, 0.9660757277303568), (3.31, 0.9645436638214051), (3.21, 0.9632304661851608), (3.1100000000000003, 0.962136134821624), (3.0100000000000002, 0.9610418034580871), (2.91, 0.960385204639965), (2.81, 0.9590720070037208), (2.71, 0.9577588093674765), (2.6100000000000003, 0.956883344276647), (2.5100000000000002, 0.9566644780039396), (2.41, 0.955132414094988), (2.31, 0.9540380827314511), (2.21, 0.9522871525497921), (2.1100000000000003, 0.9500984898227183), (2.0100000000000002, 0.9479098270956445), (1.9100000000000001, 0.9459400306412782), (1.81, 0.9430947690960823), (1.7100000000000002, 0.9398117750054716), (1.61, 0.9371853797329831), (1.51, 0.9352155832786168), (1.4100000000000001, 0.9325891880061282), (1.31, 0.927774130006566), (1.21, 0.9242722696432479), (1.11, 0.9183628802801488), (1.01, 0.9111402932808055), (0.91, 0.9043554388268767), (0.81, 0.8936309914642153), (0.7100000000000001, 0.8809367476471875), (0.61, 0.8511709345589845), (0.51, 0.8340993652878091), (0.41000000000000003, 0.807835412562924), (0.31, 0.7739111402932808), (0.21000000000000002, 0.7281680892974393), (0.11, 0.663383672576056), (0.01, 0.5773692274020573)]\n"
     ]
    }
   ],
   "source": [
    "c = {}\n",
    "train = {}\n",
    "\n",
    "for i in np.arange(0.01, 4, 0.1):\n",
    "    model_logreg = LogisticRegression(C=i)\n",
    "    model_logreg.fit(matr_train_lem, df_train['label'])\n",
    "    y_logreg = model_logreg.predict(matr_test_lem)\n",
    "    model_logreg.fit(matr_train_lem, df_train['label'])\n",
    "    accuracy = accuracy_score(df_test['label'], y_logreg)\n",
    "    y_logreg = model_logreg.predict(matr_train_lem)\n",
    "    accuracy1 = accuracy_score(df_train['label'], y_logreg)\n",
    "    \n",
    "    c[i] = accuracy \n",
    "    train[i] = accuracy1\n",
    "    \n",
    "c = sorted(c.items(), key=lambda x: x[1])\n",
    "c = c[::-1]\n",
    "train = sorted(train.items(), key=lambda x: x[1])\n",
    "train = train[::-1]\n",
    "\n",
    "\n",
    "print(c)\n",
    "print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "ba82e632",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7255416940249507\n"
     ]
    }
   ],
   "source": [
    "model_logreg = LogisticRegression(C=0.21)\n",
    "model_logreg.fit(matr_train_lem, df_lem_train['label'])\n",
    "y_logreg = model_logreg.predict(matr_train_lem)\n",
    "\n",
    "accuracy = accuracy_score(df_lem_train['label'], y_logreg)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "e3e35793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7086614173228346\n"
     ]
    }
   ],
   "source": [
    "model_logreg = LogisticRegression(C=0.21)\n",
    "model_logreg.fit(matr_train_lem, df_lem_train['label'])\n",
    "y_logreg = model_logreg.predict(matr_test_lem)\n",
    "\n",
    "accuracy = accuracy_score(df_lem_test['label'], y_logreg)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "3c5c9057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['–ª—é–±–∏—Ç—å', '–±–ª—è—Ç—å', '—Ö–æ—Ä–æ—à–∏–π', '–∫—Ä–∞—Å–∏–≤—ã–π', '–ø–∏–∑–¥–µ—Ü', '–≤–∞—É', '–º–∏–ª—ã–π',\n",
       "       '—É–º–∏—Ä–∞—Ç—å', '–Ω—Ä–∞–≤–∏—Ç—å—Å—è', '—Å—É–∫–∞'], dtype=object)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importance = model_logreg.coef_\n",
    "abs_importance = abs(importance)\n",
    "sorted_index = abs_importance.argsort()\n",
    "sorted_index = sorted_index[0][::-1]\n",
    "# —Ç–æ–ø-20 –∑–Ω–∞—á–∏–º—ã—Ö —Å–ª–æ–≤\n",
    "np.array(vec.get_feature_names_out())[sorted_index[0:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a2793ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(52, 0.7226596675415573), (40, 0.7217847769028871), (54, 0.7200349956255468), (51, 0.7165354330708661), (50, 0.7147856517935258), (48, 0.7130358705161854), (45, 0.710411198600175), (43, 0.710411198600175), (42, 0.710411198600175), (41, 0.710411198600175), (49, 0.7095363079615048), (47, 0.7086614173228346), (46, 0.7086614173228346), (53, 0.7069116360454943), (39, 0.705161854768154), (37, 0.705161854768154), (44, 0.7034120734908137), (38, 0.6990376202974629), (32, 0.6990376202974629), (33, 0.6964129483814523), (36, 0.6955380577427821), (35, 0.6937882764654418), (34, 0.6920384951881015), (28, 0.6902887139107612), (29, 0.68416447944007), (30, 0.6832895888013998), (25, 0.6824146981627297), (31, 0.6806649168853893), (26, 0.6719160104986877), (27, 0.6657917760279966)]\n",
      "[(54, 0.7793827971109653), (53, 0.7785073320201357), (52, 0.7769752681111841), (51, 0.7747866053841103), (50, 0.7728168089297439), (49, 0.7688772160210111), (47, 0.7671262858393522), (48, 0.7666885532939374), (46, 0.7660319544758153), (43, 0.7601225651127161), (42, 0.7585905012037645), (45, 0.7579339023856424), (44, 0.7570584372948129), (41, 0.7548697745677391), (39, 0.7513679142044211), (40, 0.7491792514773473), (38, 0.747209455022981), (37, 0.743707594659663), (36, 0.7434887283869556), (35, 0.7393302692055155), (34, 0.7391114029328081), (33, 0.7386736703873933), (32, 0.7332020135697089), (31, 0.730356752024513), (30, 0.7270737579339024), (29, 0.7255416940249507), (27, 0.7143795141168746), (28, 0.7132851827533377), (26, 0.706500328299409), (25, 0.7058437294812869)]\n"
     ]
    }
   ],
   "source": [
    "c = {}\n",
    "train = {}\n",
    "\n",
    "for i in range(25, 55):\n",
    "    model_random_forest = RandomForestClassifier(max_depth=i)\n",
    "    model_random_forest.fit(matr_train_lem, df_train['label'])\n",
    "    y_logreg = model_random_forest.predict(matr_test_lem)\n",
    "    model_random_forest.fit(matr_train_lem, df_train['label'])\n",
    "    accuracy = accuracy_score(df_test['label'], y_logreg)\n",
    "    y_logreg = model_random_forest.predict(matr_train_lem)\n",
    "    accuracy1 = accuracy_score(df_train['label'], y_logreg)\n",
    "    \n",
    "    c[i] = accuracy \n",
    "    train[i] = accuracy1\n",
    "    \n",
    "c = sorted(c.items(), key=lambda x: x[1])\n",
    "c = c[::-1]\n",
    "train = sorted(train.items(), key=lambda x: x[1])\n",
    "train = train[::-1]\n",
    "\n",
    "\n",
    "print(c)\n",
    "print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "20cf5632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6745406824146981\n"
     ]
    }
   ],
   "source": [
    "model_random_forest = RandomForestClassifier(max_depth=25)\n",
    "model_random_forest.fit(matr_train_lem, df_lem_train['label'])\n",
    "y_forest = model_random_forest.predict(matr_test_lem)\n",
    "\n",
    "accuracy = accuracy_score(df_lem_test['label'], y_forest)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "798e321c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6640419947506562\n"
     ]
    }
   ],
   "source": [
    "model_random_forest = RandomForestClassifier(max_depth=25)\n",
    "model_random_forest.fit(matr_train_lem, df_lem_train['label'])\n",
    "y_forest = model_random_forest.predict(matr_test_lem)\n",
    "\n",
    "accuracy = accuracy_score(df_lem_test['label'], y_forest)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "3dbd2c3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['–ª—é–±–∏—Ç—å', '–±–ª—è—Ç—å', '—Ö–æ—Ä–æ—à–∏–π', '–∫—Ä–∞—Å–∏–≤—ã–π', '–≤–∞—É', '–ø–∏–∑–¥–µ—Ü', '–º–∏–ª—ã–π',\n",
       "       '—É–º–∏—Ä–∞—Ç—å', '–Ω–∞—Ö—É–π', '—Å—É–∫–∞'], dtype=object)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importance = model_random_forest.feature_importances_\n",
    "abs_importance = abs(importance)\n",
    "sorted_index = abs_importance.argsort()[::-1]\n",
    "# —Ç–æ–ø-10 –∑–Ω–∞—á–∏–º—ã—Ö —Å–ª–æ–≤\n",
    "np.array(vec.get_feature_names_out())[sorted_index[0:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a378119c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
