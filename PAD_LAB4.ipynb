{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03f7e199",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import re\n",
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from functools import reduce\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pymorphy3\n",
    "from pymystem3 import Mystem\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "103383a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(columns=['text', 'label'])\n",
    "df1 = pd.DataFrame(columns=['text', 'label'])\n",
    "df_lem_train = pd.DataFrame(columns=['text', 'label'])\n",
    "df_lem_test = pd.DataFrame(columns=['text', 'label'])\n",
    "\n",
    "# –¥–æ—Å—Ç–∞–µ–º –æ–±—É—á–∞—é—â—É—é –≤—ã–±–æ—Ä–∫—É –∏ –∑–∞–≥—Ä—É–∂–∞–µ–º –≤ 2 –î–∞—Ç–∞–§—Ä–µ–π–º–∞: –¥–ª—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –∏ —Å—Ç–µ–º–º–∏–Ω–≥–∞\n",
    "df_train = pd.read_csv('/Users/mac/Downloads/rusentitweet_train.csv')\n",
    "for i, row in df_train.iterrows():\n",
    "    if row[1] in (\"negative\", \"positive\"):\n",
    "        df2.loc[len(df2.index)] = row\n",
    "        df_lem_train.loc[len(df_lem_train.index)] = row\n",
    "        \n",
    "df_train = df2\n",
    "\n",
    "        \n",
    "# –¥–æ—Å—Ç–∞–µ–º —Ç–µ—Å—Ç–æ–≤—É –≤—ã–±–æ—Ä–∫—É –∏ –∑–∞–≥—Ä—É–∂–∞–µ–º –≤ 2 –î–∞—Ç–∞–§—Ä–µ–π–º–∞: –¥–ª—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –∏ —Å—Ç–µ–º–º–∏–Ω–≥–∞\n",
    "df_test = pd.read_csv('/Users/mac/Downloads/rusentitweet_test.csv')\n",
    "for i, row in df_test.iterrows():\n",
    "    if row[1] in (\"negative\", \"positive\"):\n",
    "        df1.loc[len(df1.index)] = row\n",
    "        df_lem_test.loc[len(df_lem_test.index)] = row\n",
    "df_test = df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "523a81b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/mac/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "# –°—Ç–æ–ø-—Å–ª–æ–≤–∞\n",
    "stop = stopwords.words('russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3cf9dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text     label\n",
      "0                     –ø–æ–º–æ–π–º—É  –≤–∫—Ä–∞—à–∏–ª–∞—Å—å  —á–∏–º–∏–Ω–∞ ü§ß   positive\n",
      "1                                         –ø–æ—Ä—è–¥–∫–µ !   negative\n",
      "2  —Å–ª–µ–¥—É—é—â–∏–π  –±—É–¥—É  –ø–æ–±–µ–¥–Ω–æ–≥–æ  –∑–∞–∫—Ä—ã–≤–∞—Ç—å  –ø–æ–∂–µ–ª–∞–π...  positive\n",
      "3  —É–¥–∏–≤–∏—Ç–µ–ª—å–Ω—ã–π  –≥i–º–Ω  —É–¥–∏–≤–∏—Ç–µ–ª—å–Ω–æ  –ø–æ–∫–∞  –µ—â—ë  —Å–¥...  negative\n",
      "4                                   —Å—Ä–∞–ª–∞  –±–∏–æ–ª–æ–≥–∏—é   negative\n",
      "5  –ø–æ–º–∏–º–æ  –∞–ª–∏–Ω—ã  –µ—â—ë  —Ä–∞–¥–æ—Å—Ç—å  –≥–µ–º–æ–≥–ª–æ–±–∏–Ω  –ø–æ–≤—ã—Å...  positive\n",
      "6  –ø–∏–∑–¥–µ—Ü  —á—ë  –≤–æ–±—â–µ  —á—É–≤—Å—Ç–≤—É—é  –∂–∏–∑–Ω—å  –ø–æ–º–æ—Ç–∞–ª–∞  ...  negative\n",
      "7            —Å–ø–∏—á–∫–∞ ?  –∑–∞–∂–µ—á—å  –æ–≥–æ–Ω—å  —Ç–≤–æ–∏—Ö  –≥–ª–∞–∑–∞—Ö   positive\n",
      "8                    —ç—Ç–æ  —Å–∞–º–æ–º  –¥–µ–ª–µ  –æ—á–µ–Ω—å  –∫—Ä—É—Ç–æ   positive\n",
      "9  —Ö–æ—á—É  —Å–∫–∞–∑–∞—Ç—å  —á—Ç–æ—Ç–æ  –ø—Ä–∏—è—Ç–Ω–æ–µ  –ø–æ–ª—É—á–∞–µ—Ç—Å—è \"–∏–¥...  negative\n"
     ]
    }
   ],
   "source": [
    "# 2 –§—É–Ω–∫—Ü–∏—è –æ—á–∏—Å—Ç–∫–∏\n",
    "def my_clear(df):\n",
    "    s=[]\n",
    "    s1 = \"\"\n",
    "    regs = [r'https?:\\/\\/.\\S+', r'#\\S+', r'@\\S+', r'\\\\r\\\\n', r'[-.)(,:]']\n",
    "    glasn = ['–æ', '–æ', '–∞', '—è', '–∞', '–∞', '–∏', '–∞', '–µ', '—É', '–æ', '–æ', '–∞', '–µ', '—è', '–æ', '—É']\n",
    "    signs = ['!', '_', '.']\n",
    "\n",
    "    # –ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –≤—Å–µ–≥–æ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É\n",
    "    for i, row in df.iterrows():\n",
    "        row[0] = row[0].lower()\n",
    "\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        # —É–¥–∞–ª–µ–Ω–∏–µ —Å—Å—ã–ª–æ, —Ö—ç—à—Ç–µ–≥–æ–≤(–∏ –∑–Ω–∞–∫–∞ —Ä–µ—à–µ—Ç–∫–∏ –∏ –ø–æ—Å–ª–µ–¥—É—é—â–µ–≥–æ —Ö—ç—à—Ç–µ–≥–∞, –ø–æ—Ç–æ–º—É —á—Ç–æ —Å–ª–æ–≤–∞ –ø–æ—Å–ª–µ —Ö—ç—à—Ç—ç–≥–∞ –ø–∏—à—É—Ç—Å—è —Å–ª–∏—Ç–Ω–æ)\n",
    "        # —É–¥–∞–ª–µ–Ω–∏–µ –æ—Ç–º–µ—á–µ–Ω–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π(@name), —É–¥–∞–ª–µ–Ω–∏–µ –∑–Ω–∞–∫–æ–≤ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏ (–∫—Ä–æ–º–µ ! ?)\n",
    "        for reg in regs:\n",
    "            row[0] = re.sub(reg, \"\", row[0])\n",
    "\n",
    "        # —É–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤\n",
    "        row[0] = ' '.join([x for x in row[0].split() if x not in stop])\n",
    "\n",
    "        # —É–¥–∞–ª–µ–Ω–∏–µ –ø–æ–≤—Ç–æ—Ä—è—â–∏—Ö—Å—è –ø–æ–¥—Ä—è–¥ —Å–º–∞–π–ª–∏–∫–æ–≤ –∏ –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è (–æ—Å—Ç–∞–≤—à–∏—Ö—Å—è)\n",
    "        # —É–¥–∞–ª–µ–Ω–∏–µ –ø–æ–¥—Ä—è–¥ –∏–¥—É—â–∏—Ö –≥–ª–∞—Å–Ω—ã—Ö (—Ç—ã—ã—ã—ã—ã, —è—è—è—è—è—è, –∞–∞–∞–∞–∞–∞ –∏ —Ç–¥)\n",
    "        # —É–¥–∞–ª–µ–Ω–∏–µ —Ü–∏—Ñ—Ä, —Ç–∞–∫ –∫–∞–∫ –ø–æ —Å—É—Ç–∏ —Ü–∏—Ñ—Ä—ã –Ω–µ –∏–º–µ—é—Ç –Ω–∏–∫–∞–∫–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è –≤ –¥–∞–Ω–Ω–æ–º –∞–Ω–∞–ª–∏–∑–µ\n",
    "        r = row[0]\n",
    "        for t in range(0, len(row[0])):\n",
    "            if not(not(r[t].isalpha()) and r[t] == r[t-1]) and not((r[t] in glasn) and r[t] == r[t-1]) and not(r[t].isdigit()):\n",
    "                s1+=r[t]\n",
    "        row[0] = s1\n",
    "\n",
    "        s1 = \"\"\n",
    "\n",
    "        r = row[0] + \"  \"\n",
    "\n",
    "        #–æ—Ç–¥–µ–ª–µ–Ω–∏–µ —Å–º–∞–π–ª–∏–∫–∞ –æ—Ç —Å–ª–æ–≤–∞ (—á–∏–º–∏–Ω–∞ü§ß -> —á–∏–º–∏–Ω–∞ ü§ß)\n",
    "        for t in range(0, len(row[0])):\n",
    "            if not(r[t+1].isalpha()) and r[t]!= \" \" and not(r[t+1].isdigit()) and (r[t+2].isalpha() or r[t+2]==\" \"):\n",
    "                s1+=r[t] + \" \"\n",
    "            else:\n",
    "                s1+=r[t]\n",
    "        row[0] = s1\n",
    "\n",
    "        s = []\n",
    "        s1=\"\"\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb9cc3f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>–ø–æ–º–æ–π–º –≤–∫—Ä–∞—à —á–∏–º–∏–Ω ü§ß</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>–ø–æ—Ä—è–¥–∫ !</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>—Å–ª–µ–¥ –±—É–¥ –ø–æ–±–µ–¥–Ω –∑–∞–∫—Ä—ã–≤–∞ –ø–æ–∂–µ–ª–∞ —É–¥–∞—á !</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>—É–¥–∏–≤–∏—Ç–µ–ª—å–Ω –≥–∏–º–Ω —É–¥–∏–≤–∏—Ç–µ–ª—å–Ω –ø–æ–∫ –µ—â —Å–¥–æ—Ö–ª —É–∫—Ä–∞–∏–Ω</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>—Å—Ä–∞–ª –±–∏–æ–ª–æ–≥</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>–ø–æ–º–∏–º –∞–ª–∏–Ω –µ—â —Ä–∞–¥–æ—Å—Ç –≥–µ–º–æ–≥–ª–æ–±–∏–Ω –ø–æ–≤—ã—Å</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>–ø–∏–∑–¥–µ—Ü —á–µ –≤–æ–±—â —á—É–≤—Å—Ç–≤ –∂–∏–∑–Ω –ø–æ–º–æ—Ç–∞ –ø—Ä–æ—à–ª —Å—É—Ç–∫ —Ç...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>—Å–ø–∏—á–∫ ? –∑–∞–∂–µ—á –æ–≥–æ–Ω —Ç–≤–æ –≥–ª–∞–∑</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>—ç—Ç —Å–∞–º –¥–µ–ª –æ—á–µ–Ω –∫—Ä—É—Ç</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>—Ö–æ—á —Å–∫–∞–∑–∞ —á—Ç–æ—Ç –ø—Ä–∏—è—Ç–Ω –ø–æ–ª—É—á–∞ \"–∏–¥ –Ω–∞—Ö –µ–±–ª–∞ –ø—Ä–æ—Ç...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text     label\n",
       "0                               –ø–æ–º–æ–π–º –≤–∫—Ä–∞—à —á–∏–º–∏–Ω ü§ß  positive\n",
       "1                                           –ø–æ—Ä—è–¥–∫ !  negative\n",
       "2              —Å–ª–µ–¥ –±—É–¥ –ø–æ–±–µ–¥–Ω –∑–∞–∫—Ä—ã–≤–∞ –ø–æ–∂–µ–ª–∞ —É–¥–∞—á !  positive\n",
       "3     —É–¥–∏–≤–∏—Ç–µ–ª—å–Ω –≥–∏–º–Ω —É–¥–∏–≤–∏—Ç–µ–ª—å–Ω –ø–æ–∫ –µ—â —Å–¥–æ—Ö–ª —É–∫—Ä–∞–∏–Ω  negative\n",
       "4                                        —Å—Ä–∞–ª –±–∏–æ–ª–æ–≥  negative\n",
       "5              –ø–æ–º–∏–º –∞–ª–∏–Ω –µ—â —Ä–∞–¥–æ—Å—Ç –≥–µ–º–æ–≥–ª–æ–±–∏–Ω –ø–æ–≤—ã—Å  positive\n",
       "6  –ø–∏–∑–¥–µ—Ü —á–µ –≤–æ–±—â —á—É–≤—Å—Ç–≤ –∂–∏–∑–Ω –ø–æ–º–æ—Ç–∞ –ø—Ä–æ—à–ª —Å—É—Ç–∫ —Ç...  negative\n",
       "7                        —Å–ø–∏—á–∫ ? –∑–∞–∂–µ—á –æ–≥–æ–Ω —Ç–≤–æ –≥–ª–∞–∑  positive\n",
       "8                               —ç—Ç —Å–∞–º –¥–µ–ª –æ—á–µ–Ω –∫—Ä—É—Ç  positive\n",
       "9  —Ö–æ—á —Å–∫–∞–∑–∞ —á—Ç–æ—Ç –ø—Ä–∏—è—Ç–Ω –ø–æ–ª—É—á–∞ \"–∏–¥ –Ω–∞—Ö –µ–±–ª–∞ –ø—Ä–æ—Ç...  negative"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# –æ—á–∏—Å—Ç–∫–∞ –æ–±—É—á–∞—é—â–∏—Ö\n",
    "df_train = my_clear(df_train)\n",
    "\n",
    "# 3\n",
    "# —Å—Ç–µ–º–º–∏–Ω–≥ –æ–±—É—á–∞—é—â–∏—Ö\n",
    "new = SnowballStemmer(language='russian')\n",
    "for i, row in df_train.iterrows():\n",
    "    row[0] = ' '.join([new.stem(x) for x in row[0].split()])\n",
    "df_train[0:10]\n",
    "\n",
    "# —Å–æ–∑–¥–∞–Ω–∏–µ –º–∞—Å—Å–∏–≤–∞ —Å—Ç—Ä–æ–∫ –æ–±—É—á–∞—é—â–∏—Ö\n",
    "x_train = []\n",
    "for i, row in df_train.iterrows():\n",
    "    x_train.append(row[0])\n",
    "x_train[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f044dba2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['—Å—á–∏—Ç–∞ —ç—Ç –º–µ–º –≥–æ–¥',\n",
       " '—è–Ω —Ä—É—Å—Å–∫ —Å–æ—Ç–∫ –∑–∞–ø—è—Ç –Ω–∞–¥–æü§ôüèªüëçüèªüëçüèªüëçüèªüëç üèª –µ—â —Å–∏–¥–µ–ª –¥—É–º–∞ —á–æ —ç—Ç —Ç–≤–∏—Ç',\n",
       " '–ø—É—à–∫ –∫–∞—Ä–∞—É–ª—å–Ω –≥–æ—Ä —Å—Ç—Ä–µ–ª—è –±–∞–± !',\n",
       " '–º–∏–ª /—Å–º—É—Ç / —Å–ø–∞—Å–∏–± ü•∞ üå∏',\n",
       " '–±–æ–ª—å–Ω –¥—ã—à–∞—Ç –±–∞–±–∫ –ª–µ—Ç —É–∂–∞—Å üö¨',\n",
       " '–ª—é–±–ª too üíú',\n",
       " '—Å—Ç–æ–ª–æ–≤–∫ –ª—É–¥—à –≥—Ä–µ—á –º—è—Å',\n",
       " '',\n",
       " '–≤–æ–ª–Ω—É ü§ó',\n",
       " '—á—É–¥–Ω']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# –æ—á–∏—Å—Ç–∫–∞ —Ç–µ—Å—Ç\n",
    "df_test = my_clear(df_test)\n",
    "\n",
    "# 3\n",
    "# —Å—Ç–µ–º–º–∏–Ω–≥ —Ç–µ—Å—Ç–∞\n",
    "new = SnowballStemmer(language='russian')\n",
    "for i, row in df_test.iterrows():\n",
    "    row[0] = ' '.join([new.stem(x) for x in row[0].split()])\n",
    "\n",
    "# —Å–æ–∑–¥–∞–Ω–∏–µ –º–∞—Å—Å–∏–≤–∞ —Å—Ç—Ä–æ–∫ —Ç–µ—Å—Ç\n",
    "x_test = []\n",
    "for i, row in df_test.iterrows():\n",
    "    x_test.append(row[0])\n",
    "x_test[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec1978d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4\n",
    "vec = TfidfVectorizer(stop_words=None)\n",
    "\n",
    "# tf-idf test and train\n",
    "matr = vec.fit_transform(x_train)\n",
    "matr_test = vec.transform(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "450d9097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7611548556430446\n"
     ]
    }
   ],
   "source": [
    "# 5 logreg\n",
    "model_logreg = LogisticRegression()\n",
    "model_logreg.fit(matr, df_train['label'])\n",
    "y_logreg = model_logreg.predict(matr_test)\n",
    "\n",
    "accuracy = accuracy_score(df_test['label'], y_logreg)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "152b5706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3096 2796  504 ... 1903 2588 6558]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['–ª—é–±–ª', '–∫—Ä–∞—Å–∏–≤', '–±–ª—è—Ç', '–º–∏–ª', '–ª—É—á—à', '–ø–∏–∑–¥–µ—Ü', '–∫—Ä—É—Ç',\n",
       "       '–ø—Ä–µ–∫—Ä–∞—Å–Ω', '–∫–ª–∞—Å—Å–Ω', '–≤–∞', '–Ω–∞—Ö', '–Ω—Ä–∞–≤', '—Ö–æ—Ä–æ—à', '—Å—É–∫', '—Ä–∞–¥',\n",
       "       '–ª—é–±–∏–º', '–æ–±–æ–∂–∞', '–ª—é–±–æ–≤', '—Ö—É–π–Ω', '–≤–æ–±—â'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5 top logreg\n",
    "importance = model_logreg.coef_\n",
    "abs_importance = abs(importance)\n",
    "sorted_index = abs_importance.argsort()\n",
    "sorted_index = sorted_index[0][::-1]\n",
    "print(sorted_index)\n",
    "# —Ç–æ–ø-20 –∑–Ω–∞—á–∏–º—ã—Ö —Å–ª–æ–≤\n",
    "np.array(vec.get_feature_names_out())[sorted_index[0:20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "457dd5af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5774278215223098\n"
     ]
    }
   ],
   "source": [
    "# 5 random_forest\n",
    "model_random_forest = RandomForestClassifier(max_depth=1)\n",
    "model_random_forest.fit(matr, df_train['label'])\n",
    "y_forest = model_random_forest.predict(matr_test)\n",
    "\n",
    "accuracy = accuracy_score(df_test['label'], y_forest)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b825355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2861 7423 3292 ... 5234 5235    0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['–∫—Ä—É—Ç', '—Ö–æ—Ä–æ—à', '–º–∏–ª', '–æ—á–µ–Ω', '—Å–ø–∞—Å–∏–±', '–ø–æ–±–µ–¥', '—á–µ', '–µ–±–∞–Ω',\n",
       "       '–∫—Ä–∞—à', '–æ–±–æ–∂–∞', '–ø—Ä–µ–∫—Ä–∞—Å–Ω', '–≥—Ä—É—Å—Ç–Ω', '–ª—É—á—à', '—Å–æ–ª–Ω—ã—à–∫', '–Ω–∏–∫',\n",
       "       '–∫–æ–≥', '—Ç–∏–ø', '–±—Ä–æ', '–æ—Ç–ª–∏—á–Ω', '—Å–ª–æ–≤'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5 top random_forest\n",
    "importance = model_random_forest.feature_importances_\n",
    "abs_importance = abs(importance)\n",
    "sorted_index = abs_importance.argsort()[::-1]\n",
    "print(sorted_index)\n",
    "# —Ç–æ–ø-20 –∑–Ω–∞—á–∏–º—ã—Ö —Å–ª–æ–≤\n",
    "np.array(vec.get_feature_names_out())[sorted_index[0:20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504046e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f82854e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/mac/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/mac/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.stem import WordNetLemmatizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "486cbadb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text     label\n",
      "0                     –ø–æ–º–æ–π–º—É  –≤–∫—Ä–∞—à–∏–ª–∞—Å—å  —á–∏–º–∏–Ω–∞ ü§ß   positive\n",
      "1                                         –ø–æ—Ä—è–¥–∫–µ !   negative\n",
      "2  —Å–ª–µ–¥—É—é—â–∏–π  –±—É–¥—É  –ø–æ–±–µ–¥–Ω–æ–≥–æ  –∑–∞–∫—Ä—ã–≤–∞—Ç—å  –ø–æ–∂–µ–ª–∞–π...  positive\n",
      "3  —É–¥–∏–≤–∏—Ç–µ–ª—å–Ω—ã–π  –≥i–º–Ω  —É–¥–∏–≤–∏—Ç–µ–ª—å–Ω–æ  –ø–æ–∫–∞  –µ—â—ë  —Å–¥...  negative\n",
      "4                                   —Å—Ä–∞–ª–∞  –±–∏–æ–ª–æ–≥–∏—é   negative\n",
      "5  –ø–æ–º–∏–º–æ  –∞–ª–∏–Ω—ã  –µ—â—ë  —Ä–∞–¥–æ—Å—Ç—å  –≥–µ–º–æ–≥–ª–æ–±–∏–Ω  –ø–æ–≤—ã—Å...  positive\n",
      "6  –ø–∏–∑–¥–µ—Ü  —á—ë  –≤–æ–±—â–µ  —á—É–≤—Å—Ç–≤—É—é  –∂–∏–∑–Ω—å  –ø–æ–º–æ—Ç–∞–ª–∞  ...  negative\n",
      "7            —Å–ø–∏—á–∫–∞ ?  –∑–∞–∂–µ—á—å  –æ–≥–æ–Ω—å  —Ç–≤–æ–∏—Ö  –≥–ª–∞–∑–∞—Ö   positive\n",
      "8                    —ç—Ç–æ  —Å–∞–º–æ–º  –¥–µ–ª–µ  –æ—á–µ–Ω—å  –∫—Ä—É—Ç–æ   positive\n",
      "9  —Ö–æ—á—É  —Å–∫–∞–∑–∞—Ç—å  —á—Ç–æ—Ç–æ  –ø—Ä–∏—è—Ç–Ω–æ–µ  –ø–æ–ª—É—á–∞–µ—Ç—Å—è \"–∏–¥...  negative\n",
      "['–ø–æ–º–æ–π–º–∞  –≤–∫—Ä–∞—à–∏—Ç—å—Å—è  —á–∏–º–∏–Ω ü§ß', '–ø–æ—Ä—è–¥–æ–∫ !', '—Å–ª–µ–¥—É—é—â–∏–π  –±—ã—Ç—å  –ø–æ–±–µ–¥–Ω—ã–π  –∑–∞–∫—Ä—ã–≤–∞—Ç—å  –ø–æ–∂–µ–ª–∞—Ç—å  —É–¥–∞—á–∞ !', '—É–¥–∏–≤–∏—Ç–µ–ª—å–Ω—ã–π  –≥i–º–Ω  —É–¥–∏–≤–∏—Ç–µ–ª—å–Ω–æ  –ø–æ–∫–∞  –µ—â–µ  —Å–¥—ã—Ö–∞—Ç—å  —É–∫—Ä–∞–∏–Ω–∞', '—Å—Ä–∞—Ç—å  –±–∏–æ–ª–æ–≥–∏—è', '–ø–æ–º–∏–º–æ  –∞–ª–∏–Ω–∞  –µ—â–µ  —Ä–∞–¥–æ—Å—Ç—å  –≥–µ–º–æ–≥–ª–æ–±–∏–Ω  –ø–æ–≤—ã—à–∞—Ç—å—Å—è', '–ø–∏–∑–¥–µ—Ü  —á–µ  –≤–æ–±—â–µ  —á—É–≤—Å—Ç–≤–æ–≤–∞—Ç—å  –∂–∏–∑–Ω—å  –ø–æ–º–æ—Ç–∞—Ç—å  –ø—Ä–æ—à–ª—ã–π  —Å—É—Ç–∫–∏  —Ç—É–º–∞–Ω', '—Å–ø–∏—á–∫–∞ ?  –∑–∞–∂–∏–≥–∞—Ç—å  –æ–≥–æ–Ω—å  —Ç–≤–æ–π  –≥–ª–∞–∑', '—ç—Ç–æ  —Å–∞–º—ã–π  –¥–µ–ª–æ  –æ—á–µ–Ω—å  –∫—Ä—É—Ç–æ', '—Ö–æ—Ç–µ—Ç—å  —Å–∫–∞–∑–∞—Ç—å  —á—Ç–æ—Ç–æ  –ø—Ä–∏—è—Ç–Ω—ã–π  –ø–æ–ª—É—á–∞—Ç—å—Å—è \"–∏–¥—Ç–∏  –Ω–∞—Ö—É–π  –µ–±–ª–∞–Ω  –ø—Ä–æ—Ç–∏–≤–Ω—ã–π \"']\n"
     ]
    }
   ],
   "source": [
    "# clear\n",
    "df_lem_train = my_clear(df_lem_train)\n",
    "df_lem_test = my_clear(df_lem_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b389db75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mystem - –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –ª–µ–º–º–æ—Ç–∏–∑–∞—Ç–æ—Ä + pip install pymorphy3-dicts-ru - —Å–ª–æ–≤–∞—Ä–∏\n",
    "m = Mystem()\n",
    "\n",
    "x_test = []\n",
    "x_train = []\n",
    "\n",
    "# lemmatize\n",
    "for i, t in df_lem_train.iterrows():\n",
    "    lemmas = m.lemmatize(t[0])\n",
    "    x_train.append(\"\".join(lemmas).strip())\n",
    "    \n",
    "for i, t in df_lem_test.iterrows():\n",
    "    lemmas = m.lemmatize(t[0])\n",
    "    x_test.append(\"\".join(lemmas).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cae523ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = TfidfVectorizer(stop_words=None)\n",
    "\n",
    "# tf-idf test and train\n",
    "matr = vec.fit_transform(x_train)\n",
    "matr_test = vec.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "61aa77ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7681539807524059\n"
     ]
    }
   ],
   "source": [
    "model_logreg = LogisticRegression()\n",
    "model_logreg.fit(matr, df_lem_train['label'])\n",
    "y_logreg = model_logreg.predict(matr_test)\n",
    "\n",
    "accuracy = accuracy_score(df_lem_test['label'], y_logreg)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5cb34a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 530 3081 7251 ... 2013  987 2105]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['–±–ª—è—Ç—å', '–ª—é–±–∏—Ç—å', '—Ö–æ—Ä–æ—à–∏–π', '–∫—Ä–∞—Å–∏–≤—ã–π', '–≤–∞—É', '–ø–∏–∑–¥–µ—Ü', '–º–∏–ª—ã–π',\n",
       "       '—Å—É–∫–∞', '–Ω–∞—Ö—É–π', '–∫–ª–∞—Å—Å–Ω—ã–π', '–ø—Ä–µ–∫—Ä–∞—Å–Ω—ã–π', '—É–º–∏—Ä–∞—Ç—å', '–Ω—Ä–∞–≤–∏—Ç—å—Å—è',\n",
       "       '–ª—é–±–æ–≤—å', '–∫—Ä–∞—Å–∏–≤–æ', '–∫—Ä—É—Ç–æ–π', '—Ä–∞–¥', '–æ–±–æ–∂–∞—Ç—å', '–∫—Ä—É—Ç–æ', '–≤–æ–±—â–µ'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importance = model_logreg.coef_\n",
    "abs_importance = abs(importance)\n",
    "sorted_index = abs_importance.argsort()\n",
    "sorted_index = sorted_index[0][::-1]\n",
    "print(sorted_index)\n",
    "# —Ç–æ–ø-20 –∑–Ω–∞—á–∏–º—ã—Ö —Å–ª–æ–≤\n",
    "np.array(vec.get_feature_names_out())[sorted_index[0:20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f8703977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5774278215223098\n"
     ]
    }
   ],
   "source": [
    "model_random_forest = RandomForestClassifier(max_depth=1)\n",
    "model_random_forest.fit(matr, df_lem_train['label'])\n",
    "y_forest = model_random_forest.predict(matr_test)\n",
    "\n",
    "accuracy = accuracy_score(df_lem_test['label'], y_forest)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "84a7a3af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1297 5803  804 ... 5136 5137    0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['–≥–æ—Ä–¥–∏—Ç—å—Å—è', '—Å–∞–º—ã–π', '–≤–∑—è—Ç—å', '–ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å', '–∂–¥–∞—Ç—å', '—Ç—É–ø–æ–π',\n",
       "       '–≤–∞—É', '–º–∞—Ç—å', '–ø–æ–º–Ω–∏—Ç—å', '–Ω–∞–∫–æ–Ω–µ—Ü—Ç–æ', '—Ö—É–π–Ω—è', '–æ–±–æ–∂–∞—Ç—å', '–ø–ª–æ—Ö–æ',\n",
       "       '–≤–∫—É—Å–Ω–æ', '–ø–µ—Ä–≤—ã–π', '–∫—Ä–∞—à', '–Ω–∞–¥–µ—è—Ç—å—Å—è', '–∑–∞–µ–±—ã–≤–∞—Ç—å', '–∂–∞–ª—å',\n",
       "       '—Å—Ç–∞–Ω–æ–≤–∏—Ç—å—Å—è'], dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importance = model_random_forest.feature_importances_\n",
    "abs_importance = abs(importance)\n",
    "sorted_index = abs_importance.argsort()[::-1]\n",
    "print(sorted_index)\n",
    "# —Ç–æ–ø-20 –∑–Ω–∞—á–∏–º—ã—Ö —Å–ª–æ–≤\n",
    "np.array(vec.get_feature_names_out())[sorted_index[0:20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002a693d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
