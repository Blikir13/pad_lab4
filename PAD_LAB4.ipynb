{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9964443d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import re\n",
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from functools import reduce\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pymorphy3\n",
    "from pymystem3 import Mystem\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4ec33a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(columns=['text', 'label'])\n",
    "df1 = pd.DataFrame(columns=['text', 'label'])\n",
    "df_lem_train = pd.DataFrame(columns=['text', 'label'])\n",
    "df_lem_test = pd.DataFrame(columns=['text', 'label'])\n",
    "\n",
    "# –¥–æ—Å—Ç–∞–µ–º –æ–±—É—á–∞—é—â—É—é –≤—ã–±–æ—Ä–∫—É –∏ –∑–∞–≥—Ä—É–∂–∞–µ–º –≤ 2 –î–∞—Ç–∞–§—Ä–µ–π–º–∞: –¥–ª—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –∏ —Å—Ç–µ–º–º–∏–Ω–≥–∞\n",
    "df_train = pd.read_csv('/Users/mac/Downloads/rusentitweet_train.csv')\n",
    "for i, row in df_train.iterrows():\n",
    "    if row[1] in (\"negative\", \"positive\"):\n",
    "        df2.loc[len(df2.index)] = row\n",
    "        df_lem_train.loc[len(df_lem_train.index)] = row\n",
    "        \n",
    "df_train = df2\n",
    "\n",
    "        \n",
    "# –¥–æ—Å—Ç–∞–µ–º —Ç–µ—Å—Ç–æ–≤—É –≤—ã–±–æ—Ä–∫—É –∏ –∑–∞–≥—Ä—É–∂–∞–µ–º –≤ 2 –î–∞—Ç–∞–§—Ä–µ–π–º–∞: –¥–ª—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –∏ —Å—Ç–µ–º–º–∏–Ω–≥–∞\n",
    "df_test = pd.read_csv('/Users/mac/Downloads/rusentitweet_test.csv')\n",
    "for i, row in df_test.iterrows():\n",
    "    if row[1] in (\"negative\", \"positive\"):\n",
    "        df1.loc[len(df1.index)] = row\n",
    "        df_lem_test.loc[len(df_lem_test.index)] = row\n",
    "df_test = df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b10fb3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/mac/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "# –°—Ç–æ–ø-—Å–ª–æ–≤–∞\n",
    "stop = stopwords.words('russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3e9b5cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 –§—É–Ω–∫—Ü–∏—è –æ—á–∏—Å—Ç–∫–∏\n",
    "def my_clear(df):\n",
    "    s=[]\n",
    "    s1 = \"\"\n",
    "    regs = [r'https?:\\/\\/.\\S+', r'#\\S+', r'@\\S+', r'\\\\r\\\\n', r'[-.)(,:]']\n",
    "    glasn = ['–æ', '–æ', '–∞', '—è', '–∞', '–∞', '–∏', '–∞', '–µ', '—É', '–æ', '–æ', '–∞', '–µ', '—è', '–æ', '—É']\n",
    "    signs = ['!', '_', '.']\n",
    "\n",
    "    # –ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –≤—Å–µ–≥–æ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É\n",
    "    for i, row in df.iterrows():\n",
    "        row[0] = row[0].lower()\n",
    "\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        # —É–¥–∞–ª–µ–Ω–∏–µ —Å—Å—ã–ª–æ, —Ö—ç—à—Ç–µ–≥–æ–≤(–∏ –∑–Ω–∞–∫–∞ —Ä–µ—à–µ—Ç–∫–∏ –∏ –ø–æ—Å–ª–µ–¥—É—é—â–µ–≥–æ —Ö—ç—à—Ç–µ–≥–∞, –ø–æ—Ç–æ–º—É —á—Ç–æ —Å–ª–æ–≤–∞ –ø–æ—Å–ª–µ —Ö—ç—à—Ç—ç–≥–∞ –ø–∏—à—É—Ç—Å—è —Å–ª–∏—Ç–Ω–æ)\n",
    "        # —É–¥–∞–ª–µ–Ω–∏–µ –æ—Ç–º–µ—á–µ–Ω–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π(@name), —É–¥–∞–ª–µ–Ω–∏–µ –∑–Ω–∞–∫–æ–≤ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏ (–∫—Ä–æ–º–µ ! ?)\n",
    "        for reg in regs:\n",
    "            row[0] = re.sub(reg, \"\", row[0])\n",
    "\n",
    "        # —É–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤\n",
    "        row[0] = ' '.join([x for x in row[0].split() if x not in stop])\n",
    "\n",
    "        # —É–¥–∞–ª–µ–Ω–∏–µ –ø–æ–≤—Ç–æ—Ä—è—â–∏—Ö—Å—è –ø–æ–¥—Ä—è–¥ —Å–º–∞–π–ª–∏–∫–æ–≤ –∏ –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è (–æ—Å—Ç–∞–≤—à–∏—Ö—Å—è)\n",
    "        # —É–¥–∞–ª–µ–Ω–∏–µ –ø–æ–¥—Ä—è–¥ –∏–¥—É—â–∏—Ö –≥–ª–∞—Å–Ω—ã—Ö (—Ç—ã—ã—ã—ã—ã, —è—è—è—è—è—è, –∞–∞–∞–∞–∞–∞ –∏ —Ç–¥)\n",
    "        # —É–¥–∞–ª–µ–Ω–∏–µ —Ü–∏—Ñ—Ä, —Ç–∞–∫ –∫–∞–∫ –ø–æ —Å—É—Ç–∏ —Ü–∏—Ñ—Ä—ã –Ω–µ –∏–º–µ—é—Ç –Ω–∏–∫–∞–∫–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è –≤ –¥–∞–Ω–Ω–æ–º –∞–Ω–∞–ª–∏–∑–µ\n",
    "        r = row[0]\n",
    "        for t in range(0, len(row[0])):\n",
    "            if not(not(r[t].isalpha()) and r[t] == r[t-1]) and not((r[t] in glasn) and r[t] == r[t-1]) and not(r[t].isdigit()):\n",
    "                s1+=r[t]\n",
    "        row[0] = s1\n",
    "\n",
    "        s1 = \"\"\n",
    "\n",
    "        r = row[0] + \"  \"\n",
    "\n",
    "        #–æ—Ç–¥–µ–ª–µ–Ω–∏–µ —Å–º–∞–π–ª–∏–∫–∞ –æ—Ç —Å–ª–æ–≤–∞ (—á–∏–º–∏–Ω–∞ü§ß -> —á–∏–º–∏–Ω–∞ ü§ß)\n",
    "        for t in range(0, len(row[0])):\n",
    "            if not(r[t+1].isalpha()) and r[t]!= \" \" and not(r[t+1].isdigit()) and (r[t+2].isalpha() or r[t+2]==\" \"):\n",
    "                s1+=r[t] + \" \"\n",
    "            else:\n",
    "                s1+=r[t]\n",
    "        row[0] = s1\n",
    "\n",
    "        s = []\n",
    "        s1=\"\"\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "605c4dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 –æ—á–∏—Å—Ç–∫–∞ –æ–±—É—á–∞—é—â–∏—Ö\n",
    "df_train = my_clear(df_train)\n",
    "df_test = my_clear(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3f6f8d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3\n",
    "# —Å—Ç–µ–º–º–∏–Ω–≥ —Ç–µ—Å—Ç–∞\n",
    "new = SnowballStemmer(language='russian')\n",
    "for i, row in df_test.iterrows():\n",
    "    row[0] = ' '.join([new.stem(x) for x in row[0].split()])\n",
    "    \n",
    "for i, row in df_train.iterrows():\n",
    "    row[0] = ' '.join([new.stem(x) for x in row[0].split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d5206f38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['–ø–æ–º–æ–π–º –≤–∫—Ä–∞—à —á–∏–º–∏–Ω ü§ß',\n",
       " '–ø–æ—Ä—è–¥–∫ !',\n",
       " '—Å–ª–µ–¥ –±—É–¥ –ø–æ–±–µ–¥–Ω –∑–∞–∫—Ä—ã–≤–∞ –ø–æ–∂–µ–ª–∞ —É–¥–∞—á !',\n",
       " '—É–¥–∏–≤–∏—Ç–µ–ª—å–Ω –≥–∏–º–Ω —É–¥–∏–≤–∏—Ç–µ–ª—å–Ω –ø–æ–∫ –µ—â —Å–¥–æ—Ö–ª —É–∫—Ä–∞–∏–Ω',\n",
       " '—Å—Ä–∞–ª –±–∏–æ–ª–æ–≥',\n",
       " '–ø–æ–º–∏–º –∞–ª–∏–Ω –µ—â —Ä–∞–¥–æ—Å—Ç –≥–µ–º–æ–≥–ª–æ–±–∏–Ω –ø–æ–≤—ã—Å',\n",
       " '–ø–∏–∑–¥–µ—Ü —á–µ –≤–æ–±—â —á—É–≤—Å—Ç–≤ –∂–∏–∑–Ω –ø–æ–º–æ—Ç–∞ –ø—Ä–æ—à–ª —Å—É—Ç–∫ —Ç—É–º–∞–Ω',\n",
       " '—Å–ø–∏—á–∫ ? –∑–∞–∂–µ—á –æ–≥–æ–Ω —Ç–≤–æ –≥–ª–∞–∑',\n",
       " '—ç—Ç —Å–∞–º –¥–µ–ª –æ—á–µ–Ω –∫—Ä—É—Ç',\n",
       " '—Ö–æ—á —Å–∫–∞–∑–∞ —á—Ç–æ—Ç –ø—Ä–∏—è—Ç–Ω –ø–æ–ª—É—á–∞ \"–∏–¥ –Ω–∞—Ö –µ–±–ª–∞ –ø—Ä–æ—Ç–∏–≤–Ω \"']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = []\n",
    "x_test = []\n",
    "\n",
    "# —Å–æ–∑–¥–∞–Ω–∏–µ –º–∞—Å—Å–∏–≤–∞ —Å—Ç—Ä–æ–∫\n",
    "for i, row in df_train.iterrows():\n",
    "    x_train.append(row[0])\n",
    "\n",
    "for i, row in df_test.iterrows():\n",
    "    x_test.append(row[0])\n",
    "x_train[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1c814d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4\n",
    "vec = TfidfVectorizer(stop_words=None)\n",
    "\n",
    "# tf-idf test and train\n",
    "matr = vec.fit_transform(x_train)\n",
    "matr_test = vec.transform(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "235ac323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7611548556430446\n"
     ]
    }
   ],
   "source": [
    "# 5 logreg\n",
    "model_logreg = LogisticRegression()\n",
    "model_logreg.fit(matr, df_train['label'])\n",
    "y_logreg = model_logreg.predict(matr_test)\n",
    "\n",
    "accuracy = accuracy_score(df_test['label'], y_logreg)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "57133d7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['–±–ª—è—Ç—å', '–ª—é–±–∏—Ç—å', '—Ö–æ—Ä–æ—à–∏–π', '–∫—Ä–∞—Å–∏–≤—ã–π', '–≤–∞—É', '–ø–∏–∑–¥–µ—Ü', '–º–∏–ª—ã–π',\n",
       "       '—Å—É–∫–∞', '–Ω–∞—Ö—É–π', '–ø—Ä–µ–∫—Ä–∞—Å–Ω—ã–π', '–∫–ª–∞—Å—Å–Ω—ã–π', '—É–º–∏—Ä–∞—Ç—å', '–Ω—Ä–∞–≤–∏—Ç—å—Å—è',\n",
       "       '–ª—é–±–æ–≤—å', '–∫—Ä–∞—Å–∏–≤–æ', '–∫—Ä—É—Ç–æ–π', '—Ä–∞–¥', '–∫—Ä—É—Ç–æ', '–æ–±–æ–∂–∞—Ç—å', '–≤–æ–±—â–µ'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5 top logreg\n",
    "importance = model_logreg.coef_\n",
    "abs_importance = abs(importance)\n",
    "sorted_index = abs_importance.argsort()\n",
    "sorted_index = sorted_index[0][::-1]\n",
    "# —Ç–æ–ø-20 –∑–Ω–∞—á–∏–º—ã—Ö —Å–ª–æ–≤\n",
    "np.array(vec.get_feature_names_out())[sorted_index[0:20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "24e23e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7296587926509186\n"
     ]
    }
   ],
   "source": [
    "# 5 random_forest\n",
    "model_random_forest = RandomForestClassifier()\n",
    "model_random_forest.fit(matr, df_train['label'])\n",
    "y_forest = model_random_forest.predict(matr_test)\n",
    "\n",
    "accuracy = accuracy_score(df_test['label'], y_forest)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8e1e35af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['–ª—é–±–∏—Ç—å', '—Ö–æ—Ä–æ—à–∏–π', '–∫—Ä–∞—Å–∏–≤—ã–π', '–±–ª—è—Ç—å', '–≤–∞—É', '—ç—Ç–æ', '–ø–∏–∑–¥–µ—Ü',\n",
       "       '–Ω—Ä–∞–≤–∏—Ç—å—Å—è', '–º–∏–ª—ã–π', '–ª—é–±–æ–≤—å', '–∫–ª–∞—Å—Å–Ω—ã–π', '–ø—Ä–µ–∫—Ä–∞—Å–Ω—ã–π', '—Ö–æ—Ç–µ—Ç—å',\n",
       "       '–∫—Ä—É—Ç–æ–π', '–∫—Ä–∞—Å–∏–≤–æ', '—É–º–∏—Ä–∞—Ç—å', '—Ä–∞–¥', '—Å–ø–∞—Å–∏–±–æ', '—Å—É–∫–∞', '–Ω–∞—Ö—É–π'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5 top random_forest\n",
    "importance = model_random_forest.feature_importances_\n",
    "abs_importance = abs(importance)\n",
    "sorted_index = abs_importance.argsort()[::-1]\n",
    "# —Ç–æ–ø-20 –∑–Ω–∞—á–∏–º—ã—Ö —Å–ª–æ–≤\n",
    "np.array(vec.get_feature_names_out())[sorted_index[0:20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3f8867",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2d3aa3ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/mac/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/mac/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.stem import WordNetLemmatizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7c1d4f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear\n",
    "df_lem_train = my_clear(df_lem_train)\n",
    "df_lem_test = my_clear(df_lem_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8f01781d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mystem - –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –ª–µ–º–º–æ—Ç–∏–∑–∞—Ç–æ—Ä + pip install pymorphy3-dicts-ru - —Å–ª–æ–≤–∞—Ä–∏\n",
    "m = Mystem()\n",
    "\n",
    "x_test_lem = []\n",
    "x_train_lem = []\n",
    "\n",
    "# lemmatize\n",
    "for i, t in df_lem_train.iterrows():\n",
    "    lemmas = m.lemmatize(t[0])\n",
    "    x_train_lem.append(\"\".join(lemmas).strip())\n",
    "    \n",
    "for i, t in df_lem_test.iterrows():\n",
    "    lemmas = m.lemmatize(t[0])\n",
    "    x_test_lem.append(\"\".join(lemmas).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d06870de",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = TfidfVectorizer(stop_words=None)\n",
    "\n",
    "# tf-idf test and train\n",
    "matr_train_lem = vec.fit_transform(x_train_lem)\n",
    "matr_test_lem = vec.transform(x_test_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5f8577d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7742782152230971\n"
     ]
    }
   ],
   "source": [
    "model_logreg = LogisticRegression()\n",
    "model_logreg.fit(matr, df_lem_train['label'])\n",
    "y_logreg = model_logreg.predict(matr_test)\n",
    "\n",
    "accuracy = accuracy_score(df_lem_test['label'], y_logreg)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cfa985bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 530 3091 7271 ...  686 1363 2525]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['–±–ª—è—Ç—å', '–ª—é–±–∏—Ç—å', '—Ö–æ—Ä–æ—à–∏–π', '–∫—Ä–∞—Å–∏–≤—ã–π', '–≤–∞—É', '–ø–∏–∑–¥–µ—Ü', '–º–∏–ª—ã–π',\n",
       "       '—Å—É–∫–∞', '–Ω–∞—Ö—É–π', '–ø—Ä–µ–∫—Ä–∞—Å–Ω—ã–π', '–∫–ª–∞—Å—Å–Ω—ã–π', '—É–º–∏—Ä–∞—Ç—å', '–Ω—Ä–∞–≤–∏—Ç—å—Å—è',\n",
       "       '–ª—é–±–æ–≤—å', '–∫—Ä–∞—Å–∏–≤–æ', '–∫—Ä—É—Ç–æ–π', '—Ä–∞–¥', '–∫—Ä—É—Ç–æ', '–æ–±–æ–∂–∞—Ç—å', '–≤–æ–±—â–µ'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importance = model_logreg.coef_\n",
    "abs_importance = abs(importance)\n",
    "sorted_index = abs_importance.argsort()\n",
    "sorted_index = sorted_index[0][::-1]\n",
    "print(sorted_index)\n",
    "# —Ç–æ–ø-20 –∑–Ω–∞—á–∏–º—ã—Ö —Å–ª–æ–≤\n",
    "np.array(vec.get_feature_names_out())[sorted_index[0:20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "718b7a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5774278215223098\n"
     ]
    }
   ],
   "source": [
    "model_random_forest = RandomForestClassifier(max_depth=1)\n",
    "model_random_forest.fit(matr, df_lem_train['label'])\n",
    "y_forest = model_random_forest.predict(matr_test)\n",
    "\n",
    "accuracy = accuracy_score(df_lem_test['label'], y_forest)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1cd8d1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 715 3632 6193 ... 5155 5156    0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['–≤–∞—É', '–Ω–∞—Ö—É–π', '—Å–æ–Ω', '–∫—Ä–∞—Å–∏–≤—ã–π', '—Å—á–∞—Å—Ç–ª–∏–≤—ã–π', '–ø—Ä–∏—è—Ç–Ω–æ',\n",
       "       '–Ω—Ä–∞–≤–∏—Ç—å—Å—è', '–µ–±–∞–Ω—ã–π', '—Ö–∞—Ö', '—Å–æ–ª–Ω—ã—à–∫–æ', '–∫–∞–π—Ñ', '–∫—Ä—É—Ç–æ–π',\n",
       "       '—Ö–æ—Ä–æ—à–∏–π', '–∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ', '–∫–ª–∞—Å—Å–Ω–æ', '–≤–æ–±—â–µ', '–æ–±–æ–∂–∞—Ç—å', '–¥—É—à–∞',\n",
       "       '–≤–µ—Å—å', '–ø–æ–º–Ω–∏—Ç—å'], dtype=object)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importance = model_random_forest.feature_importances_\n",
    "abs_importance = abs(importance)\n",
    "sorted_index = abs_importance.argsort()[::-1]\n",
    "print(sorted_index)\n",
    "# —Ç–æ–ø-20 –∑–Ω–∞—á–∏–º—ã—Ö —Å–ª–æ–≤\n",
    "np.array(vec.get_feature_names_out())[sorted_index[0:20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112f8406",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
